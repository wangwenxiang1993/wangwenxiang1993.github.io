---
layout: post
title: DBSCAN聚类算法
date: 2018/04/13
original: true
tags: [tech, arithmetic, index]
tag: [[算法, arithmetic]]
---

##### DBSCAN(Density-Based Spatial Clustering of Applications with Noise，具有噪声的基于密度的聚类方法)是一种很典型的密度聚类算法，和K-Means，BIRCH这些一般只适用于凸样本集的聚类相比，DBSCAN既可以适用于凸样本集，也可以适用于非凸样本集。下面我们就对DBSCAN算法的原理做一个总结。
<!-- more -->

## 密度聚类原理
##### DBSCAN是一种基于密度的聚类算法，这类密度聚类算法一般假定类别可以通过样本分布的紧密程度决定。同一类别的样本，他们之间的紧密相连的，也就是说，在该类别任意样本周围不远处一定有同类别的样本存在。

##### 通过将紧密相连的样本划为一类，这样就得到了一个聚类类别。通过将所有各组紧密相连的样本划为各个不同的类别，则我们就得到了最终的所有聚类类别结果。

## DBSCAN密度定义
##### 假设我的样本集是D=(x<sub>1</sub>,x<sub>2</sub>,...,x<sub>m</sub>),则DBSCAN具体的密度描述定义如下：
1. **Eps邻域：** 对于x<sub>j</sub> ∈ D，其Eps邻域包含样本集D中与x<sub>j</sub>的距离不大于Eps的子样本集，可以简单理解成半径为Eps内的所有点的集合
2. **核心对象：** 如果对象的Eps邻域至少包含最小数目MinPts的对象，则称该对象为核心对象;
3. **边界点（edge point）：** 边界点不是核心点，但落在某个核心点的邻域内;
4. **噪音点（outlier point）：** 既不是核心点，也不是边界点的任何点;
5. **直接密度可达(directly density-reachable)：** 给定一个对象集合D，如果p在q的Eps邻域内，而q是一个核心对象，则称对象p从对象q出发时是直接密度可达的;
6. **密度可达(density-reachable)：** 如果存在一个对象链  p1, …,pi,.., pn，满足p1 = p 和pn = q，pi是从pi+1关于Eps和MinPts直接密度可达的，则对象p是从对象q关于Eps和MinPts密度可达的;

7. **密度相连(density-connected)：** 如果存在对象O∈D，使对象p和q都是从O关于Eps和MinPts密度可达的，那么对象p到q是关于Eps和MinPts密度相连的

##### 从下图可以很容易看出理解上述定义，图中MinPts=5，红色的点都是核心对象，因为其Eps邻域至少有5个样本。黑色的样本是非核心对象。所有核心对象密度直达的样本在以红色核心对象为中心的超球体内，如果不在超球体内，则不能密度直达。图中用绿色箭头连起来的核心对象组成了密度可达的样本序列。在这些密度可达的样本序列的Eps邻域内所有的样本相互都是密度相连的。

![dbscan](/img/arithmetic/dbscan.png)

## DBSCAN密度聚类思想
##### DBSCAN的聚类定义很简单：由密度可达关系导出的最大密度相连的样本集合，即为我们最终聚类的一个类别，或者说一个簇。

##### 这个DBSCAN的簇里面可以有一个或者多个核心对象。如果只有一个核心对象，则簇里其他的非核心对象样本都在这个核心对象的Eps邻域里；如果有多个核心对象，则簇里的任意一个核心对象的Eps邻域中一定有一个其他的核心对象，否则这两个核心对象无法密度可达。这些核心对象的Eps邻域里所有的样本的集合组成的一个DBSCAN聚类簇。

##### 那么怎么才能找到这样的簇样本集合呢？DBSCAN使用的方法很简单，它任意选择一个没有类别的核心对象作为种子，然后找到所有这个核心对象能够密度可达的样本集合，即为一个聚类簇。接着继续选择另一个没有类别的核心对象去寻找密度可达的样本集合，这样就得到另一个聚类簇。一直运行到所有核心对象都有类别为止。

##### 基本上这就是DBSCAN算法的主要内容了，是不是很简单？但是我们还是有三个问题没有考虑。

1. 第一个是一些异常样本点或者说少量游离于簇外的样本点，这些点不在任何一个核心对象在周围，在DBSCAN中，我们一般将这些样本点标记为噪音点。
2. 第二个是距离的度量问题，即如何计算某样本和核心对象样本的距离。在DBSCAN中，一般采用最近邻思想，采用某一种距离度量来衡量样本距离，比如欧式距离。这和KNN分类算法的最近邻思想完全相同。对应少量的样本，寻找最近邻可以直接去计算所有样本的距离，如果样本量较大，则一般采用KD树或者球树来快速的搜索最近邻。如果大家对于最近邻的思想，距离度量，KD树和球树不熟悉，可以自行百度学习。
3. 第三种问题比较特殊，某些样本可能到两个核心对象的距离都小于Eps，但是这两个核心对象由于不是密度直达，又不属于同一个聚类簇，那么如果界定这个样本的类别呢？一般来说，此时DBSCAN采用先来后到，先进行聚类的类别簇会标记这个样本为它的类别。也就是说BDSCAN的算法不是完全稳定的算法。

## python中的DBSCAN
##### 我们以sklearn.cluster.DBSCAN作为例子，做一些简单的demo：
```python
import matplotlib.pyplot as plt
from  sklearn.cluster import DBSCAN

def dbscan(data):
    result = DBSCAN(eps=2, min_samples=2).fit(data)
    components_ = result.components_
    plt.subplot(2, 1, 1)
    plt.scatter([x[0] for x in components_], [x[1] for x in components_], c=result.labels_)
    plt.subplot(2, 1, 2)
    plt.scatter([x[0] for x in data], [x[1] for x in data], c=result.labels_)
    plt.show()

if __name__ == '__main__':
    l = [
        [1,1],
        [1,2],
        [2,1],
        [2,2],
        [4,10],
        [8,2],
        [10,10],
        [10,11],
        [11,10],
        [11,11],
    ]
    dbscan(l)
```

##### 上面代码很简单，构造了一些简单的点，按照参数Eps=2和最小数目MinPts=2将点进行聚类计算。

![dbscan](/img/arithmetic/dbscanR.png)

## DBSCAN小结

##### 和传统的K-Means算法相比，DBSCAN最大的不同就是不需要输入类别数k，当然它最大的优势是可以发现任意形状的聚类簇，而不是像K-Means，一般仅仅使用于凸的样本集聚类。同时它在聚类的同时还可以找出异常点，这点和BIRCH算法类似。

##### 那么我们什么时候需要用DBSCAN来聚类呢？一般来说，如果数据集是稠密的，并且数据集不是凸的，那么用DBSCAN会比K-Means聚类效果好很多。如果数据集不是稠密的，则不推荐用DBSCAN来聚类。

#### DBSCAN的主要优点有：

1. 可以对任意形状的稠密数据集进行聚类，相对的，K-Means之类的聚类算法一般只适用于凸数据集。
2. 可以在聚类的同时发现异常点，对数据集中的异常点不敏感。
3. 聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。

#### DBSCAN的主要缺点有：
1. 如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时用DBSCAN聚类一般不适合。
2. 如果样本集较大时，聚类收敛时间较长，此时可以对搜索最近邻时建立的KD树或者球树进行规模限制来改进。
3. 调参相对于传统的K-Means之类的聚类算法稍复杂，主要需要对距离阈值Eps，邻域样本数阈值MinPts联合调参，不同的参数组合对最后的聚类效果有较大影响。